[TOC]

### 一 3D目标检测论文

#### （1）DD3D网络（CVPR2021)

论文题目：Is Pseudo-Lidar needed for Monocular 3D Object detection?

论文思路：基于FCOS网络，引入3D检测头进行计算

网络模型如下：

![image-20221018095509090](E:\HIT_SUN\HIT\md笔记\图片\image-20221018095509090.png)

在3D detection head中：

- 朝向角：$q$ 表示以3D-bbox自身为中心朝向的四元数
- 深度回归：$Z$ 表示预测深度，$Z_c$表示3D-bbox中心在z方向上的分量可以理解为它只和前景有关。$Z_q$表示像素周围的平面深度。

<img src="E:\HIT_SUN\HIT\md笔记\图片\image-20221018100103079.png" alt="image-20221018100103079" style="zoom: 80%;" />

​		f_(x/y): 表示焦距在x、y方向上以像素为单位的焦距，相当于每个焦距对应多少像素

​		( σ_l和 "μ" _l )为可学习的缩放因子和偏移量，初值来自对数据集的统计值

​		C 是常数，一般设为500。

- 中心点:$(\triangle_u,\triangle_v)$ 表示的是当前特征位置到3D边界框中心点在相机平面上的投影点的距离，可以理解为当前像素点所对应的目标的3D框中心点投影相对于当前像素点的偏移量。

- 边框：$\delta$ 表示尺寸的偏差，针对一定目标设置一个值，然后在此基础上，预测一个尺寸偏差。

预训练思路：

3个阶段：Feature学习训练→深度估计训练→3D目标检测训练

**Feature学习**：用COCO数据集预训练2D目标检测，主要是学习图像中的features，用到的heads包括：Logits、2D-box和center-ness

**深度估计(dense depth)****训练**：用DDAD15M前视相机图像预训练单目深度估计模块 → 用KITTI-Depth对PackNet进行finetune，用到的head是Dense-depth

**3D目标检测训练**： 用KITTI-3D数据集和nuScenes数据集训练3D目标检测，用到了图2中除了dense-depth之外的所有heads。

------



#### （2）Pseudo-Stereo-3D网络（CVPR2022）

论文题目：Pseudo-Stereo for Monocular 3D Object Detection in Autonomous Driving

论文思想：利用disparity map生成另个一个视角的virtual image,将两个视角特征融合后，放到检测网络中进行训练。

作者提出image to image的转换的误差要比image to LIDAR 的误差要小，深度估计的误差也会引起特征的退化。网络基本模型如下:

![image-20221018111110488](../图片/image-20221018111110488.png)

作者提出了两个基本的模型基于image level的生成和基于feature level的生成。采用**LIGA-Stereo**作为基础的检测架构，剩下的事情就是如何提取stereo view的特征了。

- **image level的生成**

  <img src="../图片/image-20221018113852392.png" alt="image-20221018113852392" style="zoom:80%;" />

为了生成virtual right view,作者首先使用**DORN**深度估计网络来预测深度，然后利用利用深度与视差的转换公式来得到视差图。
$$
d = \frac{f \cdot b}{z}
$$
其中$f$表示相机的焦距，$b$表示stereo view两个camera piar之间的基线长度，$z$表示深度。

模糊边缘、遮挡、碰撞的后处理：

To address ‘blurry’ edges, occlusions and collisions,we sharpen the disparity map by identifying the flying
pixels and applying a Sobel edge filter response of greater than 3 to the disparity map on those flying pixels 

- **feature level的生成**

因为image level的生成中forward warping是浪费时间的，所以作者提出了一个feature level 层面的stereo view 生成。如下图所示：

![image-20221018151853981](../图片/image-20221018151853981.png)

作者提出了一个新的disparity-wise dynamic convolution(DDC)来将$F_L^\prime \in R ^ {W \times H  \times C}$和$F_D \in R ^ {W \times H  \times C}$动态结合来生成$F_R^\prime \in R ^ {W \times H  \times C}$。

![image-20221018155158789](../图片/image-20221018155158789.png)

利用的公式是
$$
F_R^\prime(i,j) = \frac{1}{3\times3}\sum_{u^\prime}\sum_{v^\prime}F_D(u^\prime,v^\prime) \cdot F_L^\prime(u^\prime,v^\prime)
$$
在计算的时候，作者减少计算量，采用了一个$W \times H$的窗口，进行{-1,0,1}的平移变换。

- **feature level的复制**

![image-20221018161706052](../图片/image-20221018161706052.png)

最终的消融实验结果

![image-20221018164829138](../图片/image-20221018164829138.png)

------



#### （3）MonoFlex网络（CVPR2021)

论文题目：Objects are Different: Flexible Monocular 3D Object Detection

论文思想：CenterNet的扩展：1、增强了对图片边缘中截断物体特征提取。2、利用uncertainty estimation来评估多个深度估计值，得到最终的深度。

网络架构图如下：

![image-20221018171116857](../图片/image-20221018171116857.png)

作者指出3D目标检测的难点是寻找3D的中心点和对象的深度，因此主要改进针对这两个方面进行：

- **特征提取**

  作者将图片的object分为inside object和 outside object两种，都采用heatmap来进行表示-

  **insider object**
  $$
  \delta_{in} = \frac{x_c}{S}- \lfloor \frac{x_c}{S} \rfloor
  $$
  目的是回归$\delta_{in}$, 作者采用**CenterNet**网络中的以$x_c$为中心的圆形高斯核生成ground-truth heatmap。

  **outsider object**

$$
\delta_{in} = \frac{x_c}{S}- \lfloor \frac{x_I}{S} \rfloor
$$

​		其中$x_I$是2d中心点$x_b$与3D中心点$x_c$的连线与图边缘的焦点，如图4(a)所示

![image-20221018204857689](../图片/image-20221018204857689.png)

​			**edge fusion**

​			为了更好的利用图像边缘的信息，作者提出了一个边缘融合模块，利用mask edge提取feature map的四个		边界，顺时针组成边缘特征向量，学习边缘向量，然后再映射回去，如架构图中的edge fusion模块图。

- **2D检测**

  2D检测是运用FCOS网络

- **尺寸评估**

  采用了常用的给定大小，进行参数回归的方式

- **朝向角评估**

  采用了回归局部偏向角的方式，通过局部偏向角来反推全局偏向角,公式（5）所示。

  ![image-20221018210506757](../图片/image-20221018210506757.png)
  $$
  r_y = \alpha + arctan(x/z)
  $$

- **关键点评估**

  作者定义了10个顶点，其中八个代表3D-bbox的8个顶点，其余两个表示底面的中心点和顶面的中心点。

  ![image-20221018211211908](../图片/image-20221018211211908.png)

- **深度集成学习评估**

  **直接深度回归**
  $$
  L_{dep} = \frac{|z_r - z^*|}{\sigma_{dep}} + log(\sigma_{dep})
  $$
  **关键点求解深度**

  根据对角线我们能够生成中心点的深度，因此，作者根据10个key points生成了3组深度。
  $$
  L{kd} = \sum_{k \in c, d_1,d_2}[\frac{z_k - z^*}{\sigma_k} + I_{in}(z_k)log(\sigma_k)]
  $$
  其中，$I_{in}$表示所有的keypoints是否在图片中。

  **集成学习**
  $$
  Z_{soft} = (\sum_{i=1}^{M+1}\frac{z_i}{\sigma_i})/(\sum_{i=1}^{M+1}\frac{1}{\sigma_i})
  $$
  **整合LOSS**

  多个子任务的单独优化不能保证不同组件之间的最佳协作。作者就利用3d-bbox的8个顶点进行L1 loss的计算来整合，因为这8个点涉及到了朝向角，尺度、中心点偏移和深度

- **论文中的实验结果**![image-20221018214854270](../图片/image-20221018214854270.png)

  ------

  

#### （4）MonoGround网络（CVPR2022）

论文题目：MonoGround: Detecting Monocular 3D Objects from the Ground

论文思路: 作者在monoFLex的基础上，引入了ground地面的信息（其实是bounding box的下表面），提供更密集的深度线索。

论文模型：

![image-20221018215759638](../图片/image-20221018215759638.png)

​	作者指出从2D图像中获取3D信息困难主要有：1）在2D图像中的一个点对应着所有3D中的共线位置，相当于是一个一对多的操作，所以比较困难。2）3D物体的深度表达式与高度相关，对于两个具有相同深度和位置的物体，如果高度不同，投影的中心也就不同。（第二点感觉很勉强）

![image-20221018220844297](../图片/image-20221018220844297.png)

- **ground plane prior的先验知识**

  ​	作者将3D-bbox的下表面当作地面，然后从地面中随机选择采样点，这样这些采样点就忽略了高度的影响，

  ![image-20221018222102392](../图片/image-20221018222102392.png)
  $$
  P_{3d} = R\left[\
  	\begin{matrix}
  	k_2 - k_1 \\
  	k_4 - k_1
  	\end{matrix}
  \right] + k_1
  $$

  $$
  P_{2d}^T = K_{3 \times 3}P_{3d}^T
  $$

  ​		其中，$P_{2d}^T$可以第$i$表示为$[u_i\cdot z_i,v_i\cdot z_i, z_i ]$的形式。

  ​		作者提出了利用ground plane这样可以解决一对多的问题，使depth的深度评估更精准。

- **网络设计**

  采用MonoFlex作为baselnie。

  在3D branch中，作者大体采用了MonoFlex策略，改进点是利用offset map与10个key point全部做回归。

  ![image-20221019095614624](../图片/image-20221019095614624.png)

  在ground branch，作者采用了dilated coord convolutions 对地面的深度进行预测，作者仿照monoflex也引入了自己的关于地面的集合约束。

  

- **深度对齐**

  在ground depth的预测中，prediction map是仅仅工作在int类型的位置，projected map可以是非int类型的。为了解决这个问题，作者采用了bilinear interpolation来解决。

  ![image-20221019100942483](../图片/image-20221019100942483.png)

- **两阶段的depth inference**

  offset map与other利用3D branch求解，depth利用ground branch进行求解。

  ![image-20221019102802341](../图片/image-20221019102802341.png)

  ------

  

#### （5）MonoDTR网络(CVPR2022)

论文名称：MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer

论文思路：将transformer引入到3D检测中，提出了一个新的depth position encoding(DPE)来整合深度信息和上下文信息。

![image-20221019104222886](../图片/image-20221019104222886.png)

  backbone采用的是DLA-102。

- **depth-aware feature enhancement moudle**

  ![image-20221019111554676](../图片/image-20221019111554676.png)

  对于给定的输入特定图$F \in R^{C \times H \times W}$，利用两个卷积层来生成depth bin$D \in R^{D \times H \times W}$,D表示depth 类别（bin), depth bin采用LID方式来进行指定。

  得到$D \in R^{D \times H \times W}$后，利用GCov组卷积来降低通道数$D^\prime \in R^{D \times H \times W}$，减少运算。

  depth prototyped： $F_d$公式是：
  $$
  F_d = \sum_{i \in I}P_{di}X_i^\prime, d = { \{1,...,D^\prime \}}
  $$
  其中，其中$p_{di}$表示第$d_{th}$的均一化表示，$X^\prime_i$表示第$i_{th}$像素点的featue map，代码实现如下：

  ```python
  #depth prototype
  feat_ffm = self.conv1(feat_ffm)
  _, C, _, _ = feat_ffm.size()
  
  proj_query = coarse_x.view(N, D, -1) # N * D * (HW)
  proj_key = feat_ffm.view(N, C, -1).permute(0, 2, 1) # N * (HW) * C
  energy = torch.bmm(proj_query, proj_key)
  energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy) - energy
  attention = self.softmax(energy_new)
  ```

- **Depth positional encoding(DPE)**

  在代码实现中，DFE到DPE的传播的是depth-guide(depth bin encoding)，相当于DFE中求解得到$D^\prime$。

  ![image-20221019204320684](../图片/image-20221019204320684.png)

  这样，就可以形成深度特征，最后通过$3 \times 3$的卷积核来提取周围节点的特征。

- **transformer**

  利用encoder来学习context信息，在decoder中加入深度，利用transformer交叉注意模块的能力，有效的模拟context与depth特征之间的关系。

- **2D-3D detection and loss**

  采用YoloV3来进行预测

  -------

#### （6）MonoDETR网络（CVPR2022)

论文题目：MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection

论文思路：作者将visual特征和depth特征分开进行encode和decoder操作，在提取depth特征时，作者利用前景信息特征来进行depth map信息增强。

论文模型：

![image-20221019211311873](../图片/image-20221019211311873.png)

​	作者提出传统的方法是先寻找object的位置，比如2D检测或者是heatmap的3D中心，然后提取中心位置周围的视觉特征来获取3D-bbox的参数。所以作者认为提取特征范围太小，理解不充分。

- **feature extraction**

  作者选取**resnet-50**作为backbone提取视觉特征，a lightweight depth prediction来作为depth feature的提取器。

  **virtual feature**：采用了最大的stride=32的feature map, $f_{\frac{1}{32}} \in R^{\frac{H}{32}\times \frac{W}{32}\times C}$。  

  **depth feature**：

  ![image-20221019215846041](../图片/image-20221019215846041.png)

  ​	首先将feature map中$f_{\frac{1}{8}}$、$f_{\frac{1}{16}}$、$f_{\frac{1}{32}}$融合成$f_{\frac{1}{16}}$,然后利用两个$3 \times 3$的卷积得到depth feature $f_D$,

  ​	对于foreground depth map，我们只是生成object-wise的前景深度标签。对于在重叠的区域，我们按照最近的object depth来标注。

  ​	在深度标注的时候，采用LID的方式进行depth bin进行标注，一共分为k+1，其中k个是depth bin,还有一个是背景。depth bin划分：将foreground depth都划分到$[d_{min},d_{max}]$,最终得到划分的公式，其中$\delta$是常见的LID设置。

  ![image-20221019223549416](../图片/image-20221019223549416.png)

- **depth positional encoding**

  ```python
  #weighted_depth   对应途中的d_map(x,y)
  #depth_pos_embed  对应途中的depth positional encoding
  self.depth_pos_embed = nn.Embedding(int(self.depth_max) + 1, 256)
  
  depth_pos_embed_ip = self.interpolate_depth_embed(weighted_depth)   
      
  def interpolate_depth_embed(self, depth):
  	depth = depth.clamp(min=0, max=self.depth_max)
  	pos = self.interpolate_1d(depth, self.depth_pos_embed)
  	pos = pos.permute(0, 3, 1, 2)
  	return pos
    
  def interpolate_1d(self, coord, embed):
  	floor_coord = coord.floor()
  	delta = (coord - floor_coord).unsqueeze(-1)
  	floor_coord = floor_coord.long()
  	ceil_coord = (floor_coord + 1).clamp(max=embed.num_embeddings - 1)
  	return embed(floor_coord) * (1 - delta) + embed(ceil_coord) * delta
  ```

  **depth-guided transformer**

  将深度和视觉特征分开来encoder，利用transformer来学习全局的信息。depth encoder则是将所有的融合起来。

  在decoder中，depth cross-attention模块中，利用$Q_q = Liner(q)$ 和 $K_D, V_D = Linder(f_D)$来得到输入

  其中$Q_q \in R^{N \times C}$ 、$K_D,V_D \in R^{\frac{HW}{16^2}\times C}$ 。这样query就能更好地理解图像级别的空间关系和对象之间的几何关系。

  inter-query self-attention：**自注意力层之间的进一步特征交互，从而避免预测用一个对象的重复框**

  visual cross-attention: 收集语义信息

  **Attributes Prediction**

  利用MLP-based head预测3D-bbox的信息：类别、2D大小、3D的中心点、深度、3D-bbox大小、朝向角。

  为了更准确的评估深度，采用了dmap(x,y)的信息、2D中高度和3D中高度的约束信息，直接回归的信息来求均值。

  **BIpartite Matching**

  为了更好的将query与真实的ground-truth object进行匹配，对每个查询的标签对来计算损失，并采用**Hungarian algorithm**来寻找全局最优匹配。

  $L_{2D}$：考虑类别、2D大小，3D映射中心点。

  $L_{3D}$：考虑深度、3D大小、朝向角。

  在匹配query-label的时候，只按照$L_{2D}$。

  ------

#### （7）MonoJSG网络（CVPR2022)

论文题目：MonoJSG: Joint Semantic and Geometric Cost Volume for Monocular 3D Object Detection

论文思路：作者在Center-net网络上进行检测，然后利用adaptive cost volume机制进行后处理

论文模型：

![image-20221020211221239](../图片/image-20221020211221239.png)

  	作者将center-net网络得到的结果投影到原来图像上，会看到明显的光学误差，如下图所示。

​	作者同时提出虽然depth error能够通过原始的photometric error来实现，但是因为一些无纹理区域或者不规则区域（例如挡风玻璃和后窗等），这个策略不能很好的拓展。因此作者结合语义和几何特征，构建了一个4D cost volume，提出了一种像素级的视觉线索来细化边界框的建议。

![image-20221020214757499](../图片/image-20221020214757499.png)

- **cost column介绍**

  csdn的一个详细介绍：https://blog.csdn.net/Bruce_0712/article/details/108580474

  cost-volume在计算机视觉中特指计算机视觉中的立体匹配stereo matching问题中的一种左右视差搜索空间。

  ![image-20221020221907722](../图片/image-20221020221907722.png)

  视察的搜索，是在一定范围的，一般深度学习里会指定192，因为现实中的深度是有限的，映射回去，视差的范围也是有限的。那么视察的搜索，是在一定范围的，一般深度学习里会指定192，因为现实中的深度是有限的，映射回去，视差的范围也是有限的。

  ![img](https://pic3.zhimg.com/80/v2-ea60e630e01c1c526ffe134c1f0b5ca3_720w.webp?source=1940ef5c)

  因此，为了求两个feature map之间的相似度，我们将$(x,y)$与$(x-x_{max},y)$之间的点都进行特征比较。这样就形成了另一个维度D,从而产生了四维（D，C，H，W）。目前常用的特征比较的方法有：

  **Difference**： $（x,y）$与$(x^\prime,y)$对应的特征相减，cost columu的维度是B D C H W。

  **Concat**： $（x,y）$与$(x^\prime,y)$对应的特征叠加，cost columu的维度是B D 2C H W。

  **correlation**：$（x,y）$与$(x^\prime ,y)$对应的特征点乘，计算相关性。cost columu的维度是[B, D, H, W]

- **normalized object coordinate介绍**

  在cvpr2019年论文Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation中提出，作者使用一个共享的标准坐标空间（NOCS）作为参考系来表示同一类别中的所有物体实例.

  ![image-20221020230005434](../图片/image-20221020230005434.png)

  ![image-20221020230704444](../图片/image-20221020230704444.png)

#### （8）结果比较

![image-20221020211428697](../图片/image-20221020211428697.png)

![image-20221020211519687](../图片/image-20221020211519687.png)

![image-20221020211650359](../图片/image-20221020211650359.png)

![image-20221020211713377](../图片/image-20221020211713377.png)

![image-20221020211845674](../图片/image-20221020211845674.png)

![image-20221020211910476](../图片/image-20221020211910476.png)
